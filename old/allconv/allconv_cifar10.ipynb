{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "(32, 32, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 32, 32, 96)        2688      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 32, 32, 96)        83040     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 16, 16, 96)        83040     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 16, 16, 192)       166080    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 16, 16, 192)       331968    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 8, 8, 192)         331968    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 8, 8, 192)         331968    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 8, 8, 192)         37056     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 8, 8, 10)          1930      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,369,738\n",
      "Trainable params: 1,369,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.0193 - acc: 0.2384Epoch 00000: val_acc improved from -inf to 0.34770, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 850s - loss: 2.0192 - acc: 0.2384 - val_loss: 1.7968 - val_acc: 0.3477\n",
      "Epoch 2/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.7815 - acc: 0.3426Epoch 00001: val_acc improved from 0.34770 to 0.41410, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 818s - loss: 1.7816 - acc: 0.3426 - val_loss: 1.6152 - val_acc: 0.4141\n",
      "Epoch 3/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.6957 - acc: 0.3766Epoch 00002: val_acc improved from 0.41410 to 0.41630, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 859s - loss: 1.6957 - acc: 0.3766 - val_loss: 1.5931 - val_acc: 0.4163\n",
      "Epoch 4/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.6266 - acc: 0.4073Epoch 00003: val_acc improved from 0.41630 to 0.45490, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 862s - loss: 1.6267 - acc: 0.4072 - val_loss: 1.4939 - val_acc: 0.4549\n",
      "Epoch 5/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.5779 - acc: 0.4223Epoch 00004: val_acc improved from 0.45490 to 0.48020, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 860s - loss: 1.5778 - acc: 0.4224 - val_loss: 1.4382 - val_acc: 0.4802\n",
      "Epoch 6/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.5315 - acc: 0.4437Epoch 00005: val_acc did not improve\n",
      "1562/1562 [==============================] - 861s - loss: 1.5315 - acc: 0.4438 - val_loss: 1.4544 - val_acc: 0.4714\n",
      "Epoch 7/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.4917 - acc: 0.4569Epoch 00006: val_acc improved from 0.48020 to 0.51730, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 862s - loss: 1.4917 - acc: 0.4568 - val_loss: 1.3524 - val_acc: 0.5173\n",
      "Epoch 8/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.4460 - acc: 0.4771Epoch 00007: val_acc improved from 0.51730 to 0.51970, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 859s - loss: 1.4462 - acc: 0.4770 - val_loss: 1.3102 - val_acc: 0.5197\n",
      "Epoch 9/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.4126 - acc: 0.4907Epoch 00008: val_acc improved from 0.51970 to 0.54040, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 864s - loss: 1.4126 - acc: 0.4908 - val_loss: 1.2598 - val_acc: 0.5404\n",
      "Epoch 10/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.3637 - acc: 0.5089Epoch 00009: val_acc improved from 0.54040 to 0.55430, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 867s - loss: 1.3639 - acc: 0.5088 - val_loss: 1.2577 - val_acc: 0.5543\n",
      "Epoch 11/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.3222 - acc: 0.5258Epoch 00010: val_acc did not improve\n",
      "1562/1562 [==============================] - 860s - loss: 1.3223 - acc: 0.5258 - val_loss: 1.2825 - val_acc: 0.5495\n",
      "Epoch 12/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.2837 - acc: 0.5420Epoch 00011: val_acc improved from 0.55430 to 0.59230, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 863s - loss: 1.2837 - acc: 0.5420 - val_loss: 1.1898 - val_acc: 0.5923\n",
      "Epoch 13/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.2494 - acc: 0.5534Epoch 00012: val_acc improved from 0.59230 to 0.60150, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 861s - loss: 1.2498 - acc: 0.5533 - val_loss: 1.1450 - val_acc: 0.6015\n",
      "Epoch 14/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.2128 - acc: 0.5682Epoch 00013: val_acc improved from 0.60150 to 0.62050, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 860s - loss: 1.2127 - acc: 0.5683 - val_loss: 1.0557 - val_acc: 0.6205\n",
      "Epoch 15/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.1699 - acc: 0.5841Epoch 00014: val_acc improved from 0.62050 to 0.62960, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 858s - loss: 1.1698 - acc: 0.5841 - val_loss: 1.0456 - val_acc: 0.6296\n",
      "Epoch 16/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.1276 - acc: 0.6005Epoch 00015: val_acc improved from 0.62960 to 0.65730, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 858s - loss: 1.1276 - acc: 0.6005 - val_loss: 0.9953 - val_acc: 0.6573\n",
      "Epoch 17/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.0972 - acc: 0.6148Epoch 00016: val_acc did not improve\n",
      "1562/1562 [==============================] - 860s - loss: 1.0969 - acc: 0.6149 - val_loss: 1.0325 - val_acc: 0.6395\n",
      "Epoch 18/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.0554 - acc: 0.6294Epoch 00017: val_acc improved from 0.65730 to 0.68650, saving model to weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 857s - loss: 1.0552 - acc: 0.6294 - val_loss: 0.9419 - val_acc: 0.6865\n",
      "Epoch 19/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.0116 - acc: 0.6434Epoch 00018: val_acc did not improve\n",
      "1562/1562 [==============================] - 858s - loss: 1.0115 - acc: 0.6435 - val_loss: 1.0010 - val_acc: 0.6669\n",
      "Epoch 20/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.9776 - acc: 0.6565Epoch 00019: val_acc improved from 0.68650 to 0.70390, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 858s - loss: 0.9775 - acc: 0.6565 - val_loss: 0.8790 - val_acc: 0.7039\n",
      "Epoch 21/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.9483 - acc: 0.6682Epoch 00020: val_acc improved from 0.70390 to 0.70530, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 857s - loss: 0.9482 - acc: 0.6682 - val_loss: 0.8541 - val_acc: 0.7053\n",
      "Epoch 22/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.9188 - acc: 0.6812Epoch 00021: val_acc improved from 0.70530 to 0.72760, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 856s - loss: 0.9190 - acc: 0.6812 - val_loss: 0.7900 - val_acc: 0.7276\n",
      "Epoch 23/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.8886 - acc: 0.6910Epoch 00022: val_acc improved from 0.72760 to 0.72910, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 857s - loss: 0.8888 - acc: 0.6909 - val_loss: 0.8101 - val_acc: 0.7291\n",
      "Epoch 24/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.8623 - acc: 0.7014Epoch 00023: val_acc improved from 0.72910 to 0.74450, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 859s - loss: 0.8621 - acc: 0.7014 - val_loss: 0.7564 - val_acc: 0.7445\n",
      "Epoch 25/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.8461 - acc: 0.7066Epoch 00024: val_acc did not improve\n",
      "1562/1562 [==============================] - 853s - loss: 0.8459 - acc: 0.7066 - val_loss: 0.7833 - val_acc: 0.7381\n",
      "Epoch 26/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.8272 - acc: 0.7132Epoch 00025: val_acc did not improve\n",
      "1562/1562 [==============================] - 855s - loss: 0.8269 - acc: 0.7133 - val_loss: 0.8541 - val_acc: 0.7245\n",
      "Epoch 27/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.8085 - acc: 0.7207Epoch 00026: val_acc improved from 0.74450 to 0.75140, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 858s - loss: 0.8087 - acc: 0.7207 - val_loss: 0.7690 - val_acc: 0.7514\n",
      "Epoch 28/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.7855 - acc: 0.7275Epoch 00027: val_acc improved from 0.75140 to 0.75170, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 856s - loss: 0.7854 - acc: 0.7276 - val_loss: 0.7501 - val_acc: 0.7517\n",
      "Epoch 29/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.7709 - acc: 0.7342Epoch 00028: val_acc improved from 0.75170 to 0.75900, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 859s - loss: 0.7710 - acc: 0.7341 - val_loss: 0.7288 - val_acc: 0.7590\n",
      "Epoch 30/30\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 0.7598 - acc: 0.7372Epoch 00029: val_acc improved from 0.75900 to 0.76520, saving model to weights.hdf5\n",
      "1562/1562 [==============================] - 861s - loss: 0.7599 - acc: 0.7372 - val_loss: 0.7013 - val_acc: 0.7652\n"
     ]
    }
   ],
   "source": [
    "# adapted from https://github.com/MateLabs/All-Conv-Keras/blob/master/allconv.py\n",
    "\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Activation, Conv2D, GlobalAveragePooling2D, merge\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Lambda\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pandas\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "nb_classes = 10\n",
    "nb_epoch = 30\n",
    "\n",
    "rows, cols = 32, 32\n",
    "\n",
    "channels = 3\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "print (X_train.shape[1:])\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(96, (3, 3), padding = 'same', input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(96, (3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(96, (3, 3), padding='same', strides=(2,2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(192, (3, 3), padding = 'same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(192, (3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(192, (3, 3),padding='same', strides=(2,2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(192, (3, 3), padding = 'same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(192, (1, 1),padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(10, (1, 1), padding='valid'))\n",
    "\n",
    "\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Activation('softmax'))\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "print (model.summary())\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False) \n",
    "\n",
    "datagen.fit(X_train)\n",
    "filepath=\"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max')\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "history_callback = model.fit_generator(datagen.flow(X_train, Y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=X_train.shape[0]//batch_size,\n",
    "                        epochs=nb_epoch, validation_data=(X_test, Y_test), callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "pandas.DataFrame(history_callback.history).to_csv(\"history.csv\")\n",
    "\n",
    "model.save('keras_allconv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
