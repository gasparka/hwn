{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Caller(keras.callbacks.Callback):\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        # share max for all weights?\n",
    "        \n",
    "        w = [0, 0]\n",
    "        m = [0, 0]\n",
    "        w[0] = self.model.layers[0].get_weights()\n",
    "        m[0] = np.abs(w[0]).max()\n",
    "        \n",
    "        w[1] = self.model.layers[1].get_weights()\n",
    "        m[1] = np.abs(w[1]).max()\n",
    "        \n",
    "        mlast = np.array(m).max()\n",
    "        print('\\t ', mlast, m[0], m[1])\n",
    "        self.model.layers[0].set_weights(w[0] / m[0])\n",
    "        self.model.layers[1].set_weights(w[1] / m[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/6\n",
      "\t  0.140505 0.140505 0.0837752\n",
      "  128/60000 [..............................] - ETA: 341s - loss: 2.3084 - acc: 0.0859\t  1.00057 1.00057 1.00002\n",
      "  256/60000 [..............................] - ETA: 234s - loss: 5.7732 - acc: 0.1641\t  1.00074 1.00074 1.00074\n",
      "  384/60000 [..............................] - ETA: 207s - loss: 6.2484 - acc: 0.2005\t  1.00101 1.00043 1.00101\n",
      "  512/60000 [..............................] - ETA: 188s - loss: 6.6731 - acc: 0.2129\t  1.00022 0.999956 1.00022\n",
      "  640/60000 [..............................] - ETA: 177s - loss: 6.6173 - acc: 0.2422\t  1.00076 1.00076 1.00033\n",
      "  768/60000 [..............................] - ETA: 173s - loss: 6.8700 - acc: 0.2448\t  1.00058 1.00058 1.00002\n",
      "  896/60000 [..............................] - ETA: 167s - loss: 6.6522 - acc: 0.2746\t  1.00053 0.999679 1.00053\n",
      " 1024/60000 [..............................] - ETA: 163s - loss: 6.5650 - acc: 0.2891\t  1.00101 1.00049 1.00101\n",
      " 1152/60000 [..............................] - ETA: 158s - loss: 6.3307 - acc: 0.3056\t  1.00057 0.999273 1.00057\n",
      " 1280/60000 [..............................] - ETA: 155s - loss: 6.1395 - acc: 0.3195\t  1.00049 1.00049 0.99988\n",
      " 1408/60000 [..............................] - ETA: 152s - loss: 5.8095 - acc: 0.3331\t  1.00043 0.999757 1.00043\n",
      " 1536/60000 [..............................] - ETA: 151s - loss: 5.5441 - acc: 0.3424\t  1.00054 1.00002 1.00054\n",
      " 1664/60000 [..............................] - ETA: 151s - loss: 5.2757 - acc: 0.3528\t  1.00049 1.00049 0.999677\n",
      " 1792/60000 [..............................] - ETA: 150s - loss: 5.0355 - acc: 0.3644\t  1.00025 0.999647 1.00025\n",
      " 1920/60000 [..............................] - ETA: 148s - loss: 4.8153 - acc: 0.3734\t  1.00027 1.00027 1.00023\n",
      " 2048/60000 [>.............................] - ETA: 147s - loss: 4.6147 - acc: 0.3789\t  1.00014 1.00014 0.999881\n",
      " 2176/60000 [>.............................] - ETA: 145s - loss: 4.4211 - acc: 0.3911\t  0.999982 0.999982 0.999945\n",
      " 2304/60000 [>.............................] - ETA: 144s - loss: 4.2534 - acc: 0.3997\t  1.0002 1.0002 0.999996\n",
      " 2432/60000 [>.............................] - ETA: 142s - loss: 4.1028 - acc: 0.4108\t  1.00013 0.999731 1.00013\n",
      " 2560/60000 [>.............................] - ETA: 141s - loss: 3.9668 - acc: 0.4195\t  1.00025 1.00025 1.00019\n",
      " 2688/60000 [>.............................] - ETA: 140s - loss: 3.8351 - acc: 0.4286\t  1.00021 1.00021 1.00021\n",
      " 2816/60000 [>.............................] - ETA: 141s - loss: 3.7066 - acc: 0.4393\t  1.00039 1.00039 0.999826\n",
      " 2944/60000 [>.............................] - ETA: 140s - loss: 3.6036 - acc: 0.4440\t  1.00034 0.999589 1.00034\n",
      " 3072/60000 [>.............................] - ETA: 139s - loss: 3.5207 - acc: 0.4479\t  1.00003 0.999918 1.00003\n",
      " 3200/60000 [>.............................] - ETA: 138s - loss: 3.4369 - acc: 0.4506\t  0.999828 0.999828 0.999828\n",
      " 3328/60000 [>.............................] - ETA: 137s - loss: 3.3529 - acc: 0.4579\t  1.00004 0.999593 1.00004\n",
      " 3456/60000 [>.............................] - ETA: 137s - loss: 3.2686 - acc: 0.4638\t  1.00019 1.00002 1.00019\n",
      " 3584/60000 [>.............................] - ETA: 137s - loss: 3.1896 - acc: 0.4724\t  1.00009 0.999566 1.00009\n",
      " 3712/60000 [>.............................] - ETA: 136s - loss: 3.1244 - acc: 0.4758\t  1.00028 1.00028 0.999827\n",
      " 3840/60000 [>.............................] - ETA: 135s - loss: 3.0565 - acc: 0.4815\t  1.00017 1.00015 1.00017\n",
      " 3968/60000 [>.............................] - ETA: 135s - loss: 3.0022 - acc: 0.4849\t  0.999874 0.999752 0.999874\n",
      " 4096/60000 [=>............................] - ETA: 134s - loss: 2.9557 - acc: 0.4880\t  1.00028 1.00025 1.00028\n",
      " 4224/60000 [=>............................] - ETA: 135s - loss: 2.9074 - acc: 0.4903\t  1.00009 0.999978 1.00009\n",
      " 4352/60000 [=>............................] - ETA: 135s - loss: 2.8659 - acc: 0.4915\t  1.00025 1.00025 1.00005\n",
      " 4480/60000 [=>............................] - ETA: 135s - loss: 2.8195 - acc: 0.4958\t  1.0 1.0 0.99986\n",
      " 4608/60000 [=>............................] - ETA: 135s - loss: 2.7771 - acc: 0.4987\t  1.00049 1.00049 1.00015\n",
      " 4736/60000 [=>............................] - ETA: 135s - loss: 2.7344 - acc: 0.5017\t  1.00029 1.0 1.00029\n",
      " 4864/60000 [=>............................] - ETA: 135s - loss: 2.6933 - acc: 0.5033\t  0.999934 0.999934 0.999834\n",
      " 4992/60000 [=>............................] - ETA: 135s - loss: 2.6505 - acc: 0.5068\t  1.00037 0.999974 1.00037\n",
      " 5120/60000 [=>............................] - ETA: 135s - loss: 2.6085 - acc: 0.5115\t  1.00021 1.00021 1.00015\n",
      " 5248/60000 [=>............................] - ETA: 135s - loss: 2.5688 - acc: 0.5158\t  1.00009 0.99996 1.00009\n",
      " 5376/60000 [=>............................] - ETA: 135s - loss: 2.5289 - acc: 0.5199\t  1.00005 0.99966 1.00005\n",
      " 5504/60000 [=>............................] - ETA: 135s - loss: 2.4978 - acc: 0.5218\t  1.00003 0.999726 1.00003\n",
      " 5632/60000 [=>............................] - ETA: 134s - loss: 2.4611 - acc: 0.5265\t  1.00009 1.0 1.00009\n",
      " 5760/60000 [=>............................] - ETA: 134s - loss: 2.4247 - acc: 0.5306\t  1.00032 1.00032 1.00025\n",
      " 5888/60000 [=>............................] - ETA: 134s - loss: 2.3955 - acc: 0.5338\t  0.99976 0.999717 0.99976\n",
      " 6016/60000 [==>...........................] - ETA: 134s - loss: 2.3637 - acc: 0.5379\t  1.00036 1.00003 1.00036\n",
      " 6144/60000 [==>...........................] - ETA: 133s - loss: 2.3345 - acc: 0.5420\t  0.999957 0.999475 0.999957\n",
      " 6272/60000 [==>...........................] - ETA: 133s - loss: 2.3055 - acc: 0.5451\t  1.0004 1.0004 1.0002\n",
      " 6400/60000 [==>...........................] - ETA: 133s - loss: 2.2802 - acc: 0.5469\t  1.00004 0.999991 1.00004\n",
      " 6528/60000 [==>...........................] - ETA: 133s - loss: 2.2527 - acc: 0.5498\t  1.00031 0.999842 1.00031\n",
      " 6656/60000 [==>...........................] - ETA: 132s - loss: 2.2303 - acc: 0.5518\t  1.00014 0.99968 1.00014\n",
      " 6784/60000 [==>...........................] - ETA: 132s - loss: 2.2028 - acc: 0.5557\t  0.999863 0.999862 0.999863\n",
      " 6912/60000 [==>...........................] - ETA: 131s - loss: 2.1815 - acc: 0.5576\t  1.00032 0.999858 1.00032\n",
      " 7040/60000 [==>...........................] - ETA: 130s - loss: 2.1633 - acc: 0.5595\t  1.00038 0.999979 1.00038\n",
      " 7168/60000 [==>...........................] - ETA: 130s - loss: 2.1397 - acc: 0.5617\t  1.00012 0.99991 1.00012\n",
      " 7296/60000 [==>...........................] - ETA: 129s - loss: 2.1140 - acc: 0.5655\t  1.00021 1.00016 1.00021\n",
      " 7424/60000 [==>...........................] - ETA: 129s - loss: 2.0906 - acc: 0.5682\t  1.00009 1.00009 0.999984\n",
      " 7552/60000 [==>...........................] - ETA: 129s - loss: 2.0770 - acc: 0.5703\t  1.00027 1.00027 0.999986\n",
      " 7680/60000 [==>...........................] - ETA: 128s - loss: 2.0557 - acc: 0.5732\t  1.00021 0.999962 1.00021\n",
      " 7808/60000 [==>...........................] - ETA: 128s - loss: 2.0360 - acc: 0.5753\t  1.00017 1.00001 1.00017\n",
      " 7936/60000 [==>...........................] - ETA: 127s - loss: 2.0149 - acc: 0.5783\t  1.00006 0.999746 1.00006\n",
      " 8064/60000 [===>..........................] - ETA: 127s - loss: 1.9978 - acc: 0.5807\t  1.00025 1.00025 0.999892\n",
      " 8192/60000 [===>..........................] - ETA: 126s - loss: 1.9823 - acc: 0.5833\t  1.00026 1.00026 1.00002\n",
      " 8320/60000 [===>..........................] - ETA: 126s - loss: 1.9658 - acc: 0.5857\t  0.999971 0.999831 0.999971\n",
      " 8448/60000 [===>..........................] - ETA: 126s - loss: 1.9499 - acc: 0.5872\t  1.00035 1.00025 1.00035\n",
      " 8576/60000 [===>..........................] - ETA: 125s - loss: 1.9342 - acc: 0.5894\t  1.00001 0.999758 1.00001\n",
      " 8704/60000 [===>..........................] - ETA: 125s - loss: 1.9189 - acc: 0.5915\t  1.00014 1.00012 1.00014\n",
      " 8832/60000 [===>..........................] - ETA: 124s - loss: 1.9038 - acc: 0.5931\t  1.00015 1.00015 0.999801\n",
      " 8960/60000 [===>..........................] - ETA: 124s - loss: 1.8897 - acc: 0.5946\t  1.00019 1.00019 1.00013\n",
      " 9088/60000 [===>..........................] - ETA: 123s - loss: 1.8755 - acc: 0.5964\t  1.00036 1.00036 1.00008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9216/60000 [===>..........................] - ETA: 123s - loss: 1.8611 - acc: 0.5985\t  1.00007 0.999838 1.00007\n",
      " 9344/60000 [===>..........................] - ETA: 122s - loss: 1.8480 - acc: 0.6004\t  1.00013 0.999804 1.00013\n",
      " 9472/60000 [===>..........................] - ETA: 122s - loss: 1.8314 - acc: 0.6030\t  0.999866 0.99978 0.999866\n",
      " 9600/60000 [===>..........................] - ETA: 121s - loss: 1.8210 - acc: 0.6039\t  1.00023 0.999916 1.00023\n",
      " 9728/60000 [===>..........................] - ETA: 121s - loss: 1.8107 - acc: 0.6055\t  0.999944 0.999833 0.999944\n",
      " 9856/60000 [===>..........................] - ETA: 120s - loss: 1.7998 - acc: 0.6066\t  1.00001 0.99971 1.00001\n",
      " 9984/60000 [===>..........................] - ETA: 120s - loss: 1.7898 - acc: 0.6073\t  0.999997 0.9998 0.999997\n",
      "10112/60000 [====>.........................] - ETA: 119s - loss: 1.7764 - acc: 0.6095\t  1.00012 1.00006 1.00012\n",
      "10240/60000 [====>.........................] - ETA: 119s - loss: 1.7631 - acc: 0.6113\t  1.00007 0.999721 1.00007\n",
      "10368/60000 [====>.........................] - ETA: 118s - loss: 1.7510 - acc: 0.6133\t  1.00002 1.00002 0.999936\n",
      "10496/60000 [====>.........................] - ETA: 117s - loss: 1.7406 - acc: 0.6145\t  1.0003 0.999531 1.0003\n",
      "10624/60000 [====>.........................] - ETA: 117s - loss: 1.7324 - acc: 0.6153\t  1.0001 1.0001 0.999747\n",
      "10752/60000 [====>.........................] - ETA: 116s - loss: 1.7247 - acc: 0.6161\t  0.999989 0.999864 0.999989\n",
      "10880/60000 [====>.........................] - ETA: 115s - loss: 1.7134 - acc: 0.6176\t  1.00019 1.00007 1.00019\n",
      "11008/60000 [====>.........................] - ETA: 114s - loss: 1.7047 - acc: 0.6186\t  1.00035 1.00021 1.00035\n",
      "11136/60000 [====>.........................] - ETA: 113s - loss: 1.6957 - acc: 0.6198\t  1.00014 0.999878 1.00014\n",
      "11264/60000 [====>.........................] - ETA: 112s - loss: 1.6853 - acc: 0.6218\t  1.00011 0.999763 1.00011\n",
      "11392/60000 [====>.........................] - ETA: 111s - loss: 1.6735 - acc: 0.6235\t  1.00029 1.00008 1.00029\n",
      "11520/60000 [====>.........................] - ETA: 111s - loss: 1.6649 - acc: 0.6241\t  1.0002 1.0002 0.999985\n",
      "11648/60000 [====>.........................] - ETA: 110s - loss: 1.6546 - acc: 0.6255\t  0.999937 0.999937 0.999635\n",
      "11776/60000 [====>.........................] - ETA: 109s - loss: 1.6489 - acc: 0.6258\t  1.00007 1.00002 1.00007\n",
      "11904/60000 [====>.........................] - ETA: 108s - loss: 1.6415 - acc: 0.6270\t  1.00024 0.999929 1.00024\n",
      "12032/60000 [=====>........................] - ETA: 107s - loss: 1.6311 - acc: 0.6287\t  1.00029 1.00029 1.00028\n",
      "12160/60000 [=====>........................] - ETA: 107s - loss: 1.6232 - acc: 0.6302\t  1.00025 1.00025 1.00011\n",
      "12288/60000 [=====>........................] - ETA: 106s - loss: 1.6137 - acc: 0.6319\t  1.00032 1.00032 1.00005\n",
      "12416/60000 [=====>........................] - ETA: 105s - loss: 1.6043 - acc: 0.6335\t  1.00017 1.00017 1.00012\n",
      "12544/60000 [=====>........................] - ETA: 105s - loss: 1.5982 - acc: 0.6345\t  1.00015 0.999923 1.00015\n",
      "12672/60000 [=====>........................] - ETA: 104s - loss: 1.5887 - acc: 0.6360\t  1.00014 0.999913 1.00014\n",
      "12800/60000 [=====>........................] - ETA: 103s - loss: 1.5828 - acc: 0.6373\t  1.00027 1.00027 0.999794\n",
      "12928/60000 [=====>........................] - ETA: 102s - loss: 1.5735 - acc: 0.6389\t  1.00021 1.00021 1.0001\n",
      "13056/60000 [=====>........................] - ETA: 102s - loss: 1.5644 - acc: 0.6405\t  0.999998 0.999906 0.999998\n",
      "13184/60000 [=====>........................] - ETA: 101s - loss: 1.5600 - acc: 0.6412\t  1.00014 1.00007 1.00014\n",
      "13312/60000 [=====>........................] - ETA: 101s - loss: 1.5518 - acc: 0.6423\t  0.999992 0.999992 0.999789\n",
      "13440/60000 [=====>........................] - ETA: 100s - loss: 1.5436 - acc: 0.6438\t  1.0002 1.00018 1.0002\n",
      "13568/60000 [=====>........................] - ETA: 99s - loss: 1.5360 - acc: 0.6452 \t  1.00021 1.00021 0.999994\n",
      "13696/60000 [=====>........................] - ETA: 99s - loss: 1.5271 - acc: 0.6464\t  1.00022 1.00022 1.00007\n",
      "13824/60000 [=====>........................] - ETA: 98s - loss: 1.5181 - acc: 0.6482\t  0.999902 0.9997 0.999902\n",
      "13952/60000 [=====>........................] - ETA: 98s - loss: 1.5137 - acc: 0.6493\t  1.00027 0.999638 1.00027\n",
      "14080/60000 [======>.......................] - ETA: 97s - loss: 1.5071 - acc: 0.6503\t  0.999973 0.999923 0.999973\n",
      "14208/60000 [======>.......................] - ETA: 97s - loss: 1.4997 - acc: 0.6515\t  1.00048 0.99994 1.00048\n",
      "14336/60000 [======>.......................] - ETA: 96s - loss: 1.4926 - acc: 0.6523\t  1.0001 0.99984 1.0001\n",
      "14464/60000 [======>.......................] - ETA: 95s - loss: 1.4857 - acc: 0.6533\t  1.00026 1.00026 0.999964\n",
      "14592/60000 [======>.......................] - ETA: 95s - loss: 1.4774 - acc: 0.6545\t  1.00009 1.00009 1.00008\n",
      "14720/60000 [======>.......................] - ETA: 94s - loss: 1.4699 - acc: 0.6559\t  1.00025 1.00025 1.00021\n",
      "14848/60000 [======>.......................] - ETA: 94s - loss: 1.4641 - acc: 0.6567\t  1.00012 1.00012 1.00004\n",
      "14976/60000 [======>.......................] - ETA: 93s - loss: 1.4570 - acc: 0.6579\t  1.00016 1.00014 1.00016\n",
      "15104/60000 [======>.......................] - ETA: 93s - loss: 1.4506 - acc: 0.6588\t  1.00043 1.00043 0.999717\n",
      "15232/60000 [======>.......................] - ETA: 92s - loss: 1.4461 - acc: 0.6591\t  1.00002 0.999524 1.00002\n",
      "15360/60000 [======>.......................] - ETA: 92s - loss: 1.4408 - acc: 0.6596\t  1.00007 0.999819 1.00007\n",
      "15488/60000 [======>.......................] - ETA: 91s - loss: 1.4338 - acc: 0.6611\t  1.00007 0.999823 1.00007\n",
      "15616/60000 [======>.......................] - ETA: 91s - loss: 1.4266 - acc: 0.6621\t  1.00002 0.999658 1.00002\n",
      "15744/60000 [======>.......................] - ETA: 90s - loss: 1.4207 - acc: 0.6629\t  1.00017 1.00017 1.00002\n",
      "15872/60000 [======>.......................] - ETA: 90s - loss: 1.4148 - acc: 0.6641\t  1.0002 0.999845 1.0002\n",
      "16000/60000 [=======>......................] - ETA: 89s - loss: 1.4084 - acc: 0.6653\t  1.00008 0.999853 1.00008\n",
      "16128/60000 [=======>......................] - ETA: 89s - loss: 1.4021 - acc: 0.6662\t  1.00013 1.00013 1.00009\n",
      "16256/60000 [=======>......................] - ETA: 88s - loss: 1.3965 - acc: 0.6671\t  1.00009 0.999654 1.00009\n",
      "16384/60000 [=======>......................] - ETA: 88s - loss: 1.3902 - acc: 0.6682\t  1.00031 1.00031 0.99987\n",
      "16512/60000 [=======>......................] - ETA: 87s - loss: 1.3862 - acc: 0.6690\t  1.00031 1.00005 1.00031\n",
      "16640/60000 [=======>......................] - ETA: 87s - loss: 1.3807 - acc: 0.6701\t  1.00022 1.00022 1.00011\n",
      "16768/60000 [=======>......................] - ETA: 86s - loss: 1.3775 - acc: 0.6710\t  1.00025 1.00025 1.00016\n",
      "16896/60000 [=======>......................] - ETA: 86s - loss: 1.3737 - acc: 0.6716\t  1.0002 1.00017 1.0002\n",
      "17024/60000 [=======>......................] - ETA: 86s - loss: 1.3674 - acc: 0.6729\t  1.00072 1.00072 1.0001\n",
      "17152/60000 [=======>......................] - ETA: 85s - loss: 1.3629 - acc: 0.6737\t  1.00012 0.999801 1.00012\n",
      "17280/60000 [=======>......................] - ETA: 85s - loss: 1.3574 - acc: 0.6748\t  1.00034 0.99994 1.00034\n",
      "17408/60000 [=======>......................] - ETA: 85s - loss: 1.3507 - acc: 0.6762\t  1.00024 0.999776 1.00024\n",
      "17536/60000 [=======>......................] - ETA: 85s - loss: 1.3465 - acc: 0.6772\t  0.999988 0.999714 0.999988\n",
      "17664/60000 [=======>......................] - ETA: 84s - loss: 1.3412 - acc: 0.6782\t  1.00032 1.00032 1.0\n",
      "17792/60000 [=======>......................] - ETA: 84s - loss: 1.3363 - acc: 0.6790\t  1.00007 0.999879 1.00007\n",
      "17920/60000 [=======>......................] - ETA: 83s - loss: 1.3308 - acc: 0.6800\t  1.00029 1.00029 0.999965\n",
      "18048/60000 [========>.....................] - ETA: 83s - loss: 1.3245 - acc: 0.6812\t  1.00031 1.0001 1.00031\n",
      "18176/60000 [========>.....................] - ETA: 82s - loss: 1.3195 - acc: 0.6822\t  1.00044 1.00044 1.00002\n",
      "18304/60000 [========>.....................] - ETA: 82s - loss: 1.3143 - acc: 0.6833\t  1.00017 0.999925 1.00017\n",
      "18432/60000 [========>.....................] - ETA: 81s - loss: 1.3080 - acc: 0.6845\t  1.00026 0.999958 1.00026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18560/60000 [========>.....................] - ETA: 81s - loss: 1.3046 - acc: 0.6853\t  1.00016 0.999861 1.00016\n",
      "18688/60000 [========>.....................] - ETA: 80s - loss: 1.2996 - acc: 0.6861\t  1.00008 0.999833 1.00008\n",
      "18816/60000 [========>.....................] - ETA: 80s - loss: 1.2938 - acc: 0.6872\t  1.00034 0.999738 1.00034\n",
      "18944/60000 [========>.....................] - ETA: 79s - loss: 1.2887 - acc: 0.6883\t  1.00009 0.999915 1.00009\n",
      "19072/60000 [========>.....................] - ETA: 79s - loss: 1.2841 - acc: 0.6892\t  1.00014 1.00014 0.999806\n",
      "19200/60000 [========>.....................] - ETA: 78s - loss: 1.2811 - acc: 0.6898\t  1.00014 0.999912 1.00014\n",
      "19328/60000 [========>.....................] - ETA: 78s - loss: 1.2756 - acc: 0.6910\t  1.0003 1.0003 1.00022\n",
      "19456/60000 [========>.....................] - ETA: 78s - loss: 1.2713 - acc: 0.6917\t  1.00003 0.999805 1.00003\n",
      "19584/60000 [========>.....................] - ETA: 77s - loss: 1.2662 - acc: 0.6927\t  1.00029 1.00029 1.00021\n",
      "19712/60000 [========>.....................] - ETA: 77s - loss: 1.2611 - acc: 0.6935\t  1.00001 1.00001 0.999992\n",
      "19840/60000 [========>.....................] - ETA: 76s - loss: 1.2583 - acc: 0.6942\t  1.00008 0.999997 1.00008\n",
      "19968/60000 [========>.....................] - ETA: 76s - loss: 1.2542 - acc: 0.6952\t  1.00017 1.00017 0.999882\n",
      "20096/60000 [=========>....................] - ETA: 75s - loss: 1.2517 - acc: 0.6956\t  1.00011 1.00003 1.00011\n",
      "20224/60000 [=========>....................] - ETA: 75s - loss: 1.2489 - acc: 0.6958\t  1.00017 1.00009 1.00017\n",
      "20352/60000 [=========>....................] - ETA: 75s - loss: 1.2438 - acc: 0.6969\t  1.00034 1.00034 1.00012\n",
      "20480/60000 [=========>....................] - ETA: 74s - loss: 1.2399 - acc: 0.6976\t  1.00018 1.00018 0.999916\n",
      "20608/60000 [=========>....................] - ETA: 74s - loss: 1.2360 - acc: 0.6983\t  1.00003 1.00003 0.999885\n",
      "20736/60000 [=========>....................] - ETA: 73s - loss: 1.2327 - acc: 0.6991\t  1.00011 1.00004 1.00011\n",
      "20864/60000 [=========>....................] - ETA: 73s - loss: 1.2277 - acc: 0.7001\t  0.999982 0.999982 0.999856\n",
      "20992/60000 [=========>....................] - ETA: 73s - loss: 1.2241 - acc: 0.7005\t  1.00007 0.99995 1.00007\n",
      "21120/60000 [=========>....................] - ETA: 72s - loss: 1.2201 - acc: 0.7014\t  1.00006 1.00006 1.00004\n",
      "21248/60000 [=========>....................] - ETA: 72s - loss: 1.2154 - acc: 0.7021\t  1.00005 0.999903 1.00005\n",
      "21376/60000 [=========>....................] - ETA: 71s - loss: 1.2121 - acc: 0.7029"
     ]
    }
   ],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 6\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape, use_bias=False))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', use_bias=False))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[Caller()])\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fcd597aef08b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gaspar/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# small reductions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "w = [0, 0]\n",
    "m = [0, 0]\n",
    "w[0] = model.layers[0].get_weights()\n",
    "m[0] = np.abs(w[0]).max()\n",
    "\n",
    "w[1] = model.layers[1].get_weights()\n",
    "m[1] = np.abs(w[1]).max()\n",
    "\n",
    "mlast = np.array(m).max()\n",
    "print('\\t ', mlast, m[0], m[1])\n",
    "self.model.layers[0].set_weights(w[0] / mlast)\n",
    "self.model.layers[1].set_weights(w[1] / mlast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
